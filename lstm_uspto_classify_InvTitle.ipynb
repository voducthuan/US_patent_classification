{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3000 # size of vocabulary\n",
    "embedding_dim = 64\n",
    "max_length = 20\n",
    "training_portion = .85 # set ratio of train (85%) and validation (15%)\n",
    "\n",
    "list_of_patents = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12343\n",
      "12343\n"
     ]
    }
   ],
   "source": [
    "# Read data and remove stopword\n",
    "with open(\"basf_challenge/data/uspto.csv\", 'r', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[3])\n",
    "        patent = row[2]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            patent = patent.replace(token, ' ')\n",
    "            patent = patent.replace(' ', ' ')\n",
    "        list_of_patents.append(patent)\n",
    "print(len(labels))\n",
    "print(len(list_of_patents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'method': 2,\n",
       " 'device': 3,\n",
       " 'system': 4,\n",
       " 'apparatus': 5,\n",
       " 'methods': 6,\n",
       " 'systems': 7,\n",
       " 'using': 8,\n",
       " 'control': 9,\n",
       " 'same': 10,\n",
       " 'vehicle': 11,\n",
       " 'processing': 12,\n",
       " 'display': 13,\n",
       " 'thereof': 14,\n",
       " 'based': 15,\n",
       " 'data': 16,\n",
       " 'image': 17,\n",
       " 'manufacturing': 18,\n",
       " 'assembly': 19,\n",
       " 'storage': 20}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(list_of_patents) * training_portion)\n",
    "train_patents = list_of_patents[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "validation_patents = list_of_patents[train_size:]\n",
    "validation_labels = labels[train_size:]\n",
    "\n",
    "oov_tok = '<OOV>'\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_patents)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "dict(list(word_index.items())[0:20]) ## print out first 20 index of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 9, 2, 14, 12, 5, 12, 2]\n",
      "[1, 8, 581, 1, 2621]\n",
      "[1, 64, 4, 2, 78, 2622, 3]\n",
      "[4, 42, 2, 14, 228, 643, 582]\n",
      "[310, 2198, 582, 2199, 4]\n",
      "[24, 5, 2, 42, 10, 43, 124, 98, 38]\n",
      "[975, 677, 33, 222, 229, 898, 134, 1297]\n",
      "[1076, 63, 1168]\n",
      "[2, 46, 395, 2623, 468]\n",
      "[170, 112, 91, 490, 6, 1446, 62, 7]\n",
      "[21, 6, 7, 716, 30, 2200, 1447]\n",
      "[1169, 717, 62, 355, 4]\n",
      "[831, 1876, 39, 92, 3, 831, 1876, 39, 92, 2]\n",
      "[1, 15, 338, 1, 339, 976]\n",
      "[2, 5, 246, 2624, 2201, 1, 448, 1643, 251, 8, 1, 2625, 644, 718, 770]\n",
      "[5, 2, 93, 1877, 505]\n",
      "[1878, 1, 1, 1]\n",
      "[24, 3, 1, 719, 16]\n",
      "[1, 469, 85, 112, 340]\n",
      "[645, 85, 33]\n",
      "[4, 2, 1, 1076, 551, 449]\n",
      "[230, 678, 36, 7, 6]\n",
      "[5, 2, 93, 719, 976, 2626]\n",
      "[118, 1, 470, 8, 192, 2627, 62]\n",
      "[1877, 70, 252, 5, 18, 2, 14]\n",
      "[4, 2, 93, 1, 1, 1, 679, 2628, 2629, 1]\n",
      "[1, 3, 6]\n",
      "[832, 4, 149, 118]\n",
      "[4, 2, 93, 287, 1076, 1879, 1880]\n",
      "[1, 15, 1881, 44, 125, 5, 2, 1448, 506, 1, 8, 1, 583, 1]\n",
      "[396, 899, 64, 8, 1882, 900]\n",
      "[6, 92, 1, 2202, 1644, 8, 901]\n",
      "[1449, 1, 680, 112, 1, 1298, 62, 6]\n",
      "[902, 1, 1, 1077, 163, 1645, 643, 1298]\n",
      "[2630, 677, 147, 260, 833, 7, 6, 2631, 79, 374, 1]\n",
      "[173, 1, 2203, 1170, 1298, 1, 62]\n",
      "[1, 720, 646, 19]\n",
      "[1171, 19, 29, 903, 79, 72]\n",
      "[1450, 450, 451, 611, 171, 134]\n",
      "[904, 3, 4, 1078, 2, 834, 904]\n",
      "[21, 7, 6, 8, 63, 1883, 142, 1]\n",
      "[904, 2204, 7, 2632, 95]\n",
      "[78, 1, 15, 1, 298]\n",
      "[4, 2, 63, 60, 397, 977, 2205, 1, 64]\n",
      "[2633, 25, 4, 2]\n",
      "[2, 4, 63, 2206, 1, 771, 1, 2206]\n",
      "[978, 280, 4, 2, 14]\n",
      "[423, 979, 4, 8, 1, 33]\n",
      "[4, 2, 63, 1646, 2634, 1]\n",
      "[1, 2207, 33]\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_patents)\n",
    "# First of 50 records in token form\n",
    "for i in range(50):\n",
    "    print(train_sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  5  9  2 14 12  5 12  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[   1    8  581    1 2621    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1   64    4    2   78 2622    3    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  4  42   2  14 228 643 582   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[ 310 2198  582 2199    4    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 24   5   2  42  10  43 124  98  38   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[ 975  677   33  222  229  898  134 1297    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[1076   63 1168    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2   46  395 2623  468    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 170  112   91  490    6 1446   62    7    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  21    6    7  716   30 2200 1447    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[1169  717   62  355    4    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 831 1876   39   92    3  831 1876   39   92    2    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  1  15 338   1 339 976   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   2    5  246 2624 2201    1  448 1643  251    8    1 2625  644  718\n",
      "  770    0    0    0    0    0]\n",
      "[   5    2   93 1877  505    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[1878    1    1    1    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 24   3   1 719  16   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[  1 469  85 112 340   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[645  85  33   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   4    2    1 1076  551  449    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[230 678  36   7   6   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   5    2   93  719  976 2626    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 118    1  470    8  192 2627   62    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[1877   70  252    5   18    2   14    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   4    2   93    1    1    1  679 2628 2629    1    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[832   4 149 118   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   4    2   93  287 1076 1879 1880    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1   15 1881   44  125    5    2 1448  506    1    8    1  583    1\n",
      "    0    0    0    0    0    0]\n",
      "[ 396  899   64    8 1882  900    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   6   92    1 2202 1644    8  901    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[1449    1  680  112    1 1298   62    6    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 902    1    1 1077  163 1645  643 1298    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[2630  677  147  260  833    7    6 2631   79  374    1    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 173    1 2203 1170 1298    1   62    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  1 720 646  19   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[1171   19   29  903   79   72    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[1450  450  451  611  171  134    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 904    3    4 1078    2  834  904    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  21    7    6    8   63 1883  142    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 904 2204    7 2632   95    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 78   1  15   1 298   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   4    2   63   60  397  977 2205    1   64    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[2633   25    4    2    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2    4   63 2206    1  771    1 2206    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[978 280   4   2  14   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[423 979   4   8   1  33   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   4    2   63 1646 2634    1    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 2207   33    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "# First of 50 records after padding to size 20\n",
    "for i in range(50):\n",
    "    print(train_padded[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(validation_patents)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G', 'F', 'A', 'H', 'C', 'D', 'E', 'B'}\n"
     ]
    }
   ],
   "source": [
    "# set of lables\n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to token\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "# First of 20 labels (token form)\n",
    "for i in range(50):\n",
    "    print(training_label_seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "system method <OOV> sleep stage classification ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "System method cardiorespiratory sleep stage classification\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Checking encode and original\n",
    "def decode_patent(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print('------------------------')\n",
    "print(decode_patent(train_padded[20]))\n",
    "print(train_patents[20])\n",
    "print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          192000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 266,889\n",
      "Trainable params: 266,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(9, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10491 samples, validate on 1852 samples\n",
      "Epoch 1/15\n",
      "10491/10491 - 9s - loss: 1.5166 - accuracy: 0.4279 - val_loss: 1.1659 - val_accuracy: 0.4455\n",
      "Epoch 2/15\n",
      "10491/10491 - 5s - loss: 1.0291 - accuracy: 0.6301 - val_loss: 1.0513 - val_accuracy: 0.5454\n",
      "Epoch 3/15\n",
      "10491/10491 - 5s - loss: 0.8080 - accuracy: 0.7166 - val_loss: 1.2848 - val_accuracy: 0.5005\n",
      "Epoch 4/15\n",
      "10491/10491 - 5s - loss: 0.6766 - accuracy: 0.7652 - val_loss: 1.1255 - val_accuracy: 0.5616\n",
      "Epoch 5/15\n",
      "10491/10491 - 5s - loss: 0.5798 - accuracy: 0.7963 - val_loss: 1.3148 - val_accuracy: 0.5227\n",
      "Epoch 6/15\n",
      "10491/10491 - 5s - loss: 0.5068 - accuracy: 0.8221 - val_loss: 1.4983 - val_accuracy: 0.5232\n",
      "Epoch 7/15\n",
      "10491/10491 - 5s - loss: 0.4458 - accuracy: 0.8444 - val_loss: 1.5539 - val_accuracy: 0.5443\n",
      "Epoch 8/15\n",
      "10491/10491 - 4s - loss: 0.3970 - accuracy: 0.8589 - val_loss: 1.5008 - val_accuracy: 0.5934\n",
      "Epoch 9/15\n",
      "10491/10491 - 4s - loss: 0.3508 - accuracy: 0.8747 - val_loss: 1.8177 - val_accuracy: 0.5432\n",
      "Epoch 10/15\n",
      "10491/10491 - 5s - loss: 0.3181 - accuracy: 0.8838 - val_loss: 2.2523 - val_accuracy: 0.5124\n",
      "Epoch 11/15\n",
      "10491/10491 - 4s - loss: 0.2864 - accuracy: 0.8995 - val_loss: 1.8372 - val_accuracy: 0.5907\n",
      "Epoch 12/15\n",
      "10491/10491 - 4s - loss: 0.2603 - accuracy: 0.9077 - val_loss: 2.4235 - val_accuracy: 0.5302\n",
      "Epoch 13/15\n",
      "10491/10491 - 4s - loss: 0.2440 - accuracy: 0.9113 - val_loss: 2.4033 - val_accuracy: 0.5535\n",
      "Epoch 14/15\n",
      "10491/10491 - 5s - loss: 0.2191 - accuracy: 0.9212 - val_loss: 2.6884 - val_accuracy: 0.5232\n",
      "Epoch 15/15\n",
      "10491/10491 - 4s - loss: 0.2048 - accuracy: 0.9240 - val_loss: 2.7462 - val_accuracy: 0.5221\n"
     ]
    }
   ],
   "source": [
    "# Traing model with 15 epochs\n",
    "num_epochs = 15\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00608218 0.18759827 0.1466277  0.18126073 0.15570012 0.20532756\n",
      "  0.06251923 0.04249167 0.01239252]]\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "# Predict input text\n",
    "patent_input = [\"Apparatus and method for determining a physiological condition\"]\n",
    "seq = tokenizer.texts_to_sequences(patent_input)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "prediction = model.predict(padded)\n",
    "print(prediction)\n",
    "print(labels[np.argmax(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
